{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "089f7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b6b404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "wine = pd.read_csv(\"winequality-red.csv\")\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13164fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78641800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.isnull().sum()\n",
    "type(wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b8776",
   "metadata": {},
   "source": [
    "## rename을 사용해서 특성명에 존재하는 공백을 ' _ '로 대체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a29be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed_acidity         1599 non-null   float64\n",
      " 1   volatile_acidity      1599 non-null   float64\n",
      " 2   citric_acid           1599 non-null   float64\n",
      " 3   residual_sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free_sulfur_dioxide   1599 non-null   float64\n",
      " 6   total_sulfur_dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "wine.rename(columns={'fixed acidity' : 'fixed_acidity', 'volatile acidity': 'volatile_acidity', \n",
    "                    'citric acid': 'citric_acid', 'residual sugar': 'residual_sugar', \n",
    "                    'free sulfur dioxide': 'free_sulfur_dioxide', 'total sulfur dioxide': 'total_sulfur_dioxide'}, inplace=True)\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62eb4f",
   "metadata": {},
   "source": [
    "### 각 특성이 quality와 상관관계(?)에서 alcohol, sulphates, citric_acid, fixed_acidity (추가로, residual_sugar)를 제외하고 음의 상관관계를 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c0d41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality                 1.000000\n",
       "alcohol                 0.476166\n",
       "sulphates               0.251397\n",
       "citric_acid             0.226373\n",
       "fixed_acidity           0.124052\n",
       "residual_sugar          0.013732\n",
       "free_sulfur_dioxide    -0.050656\n",
       "pH                     -0.057731\n",
       "chlorides              -0.128907\n",
       "density                -0.174919\n",
       "total_sulfur_dioxide   -0.185100\n",
       "volatile_acidity       -0.390558\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation = wine.corr()\n",
    "correlation['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d80930",
   "metadata": {},
   "source": [
    "# Shuffle !!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6adb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shuffle = wine.iloc[np.random.permutation(wine.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9279460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shuffle = matrix_shuffle[\"quality\"]\n",
    "X_shuffle = matrix_shuffle.drop([\"quality\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abbf166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol        0.476166\n",
       "sulphates      0.251397\n",
       "citric_acid    0.226373\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = correlation['quality'].sort_values(ascending=False)\n",
    "X_train_new = X_train_new[1:4]\n",
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3406e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine['rate_code'] = (wine['quality'] > 4).astype(np.float32)\n",
    "# wine['rate_code'] = wine['quality'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eae5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegression(max_iter = 5000, random_state=42)\n",
    "y_train_new, X = dmatrices('quality ~ alcohol + sulphates + citric_acid + fixed_acidity', data = wine)\n",
    "# model.fit(X, y.ravel())\n",
    "# cross_val_score(model, X, y.ravel(), cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4280d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.565986635"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.43125   , 0.5375    , 0.5875    , 0.49375   , 0.6       ,\n",
    "       0.69375   , 0.61875   , 0.49375   , 0.625     , 0.57861635])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b94244",
   "metadata": {},
   "source": [
    "### 특성 중 상관계수가 ±0.0대인것들 제외하고 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc5edc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new, X_new = dmatrices('quality ~ alcohol + sulphates + citric_acid + fixed_acidity + residual_sugar + chlorides + density + total_sulfur_dioxide + volatile_acidity', data=wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e223fc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (1599, 10)\n",
       "  Columns:\n",
       "    ['Intercept',\n",
       "     'alcohol',\n",
       "     'sulphates',\n",
       "     'citric_acid',\n",
       "     'fixed_acidity',\n",
       "     'residual_sugar',\n",
       "     'chlorides',\n",
       "     'density',\n",
       "     'total_sulfur_dioxide',\n",
       "     'volatile_acidity']\n",
       "  Terms:\n",
       "    'Intercept' (column 0)\n",
       "    'alcohol' (column 1)\n",
       "    'sulphates' (column 2)\n",
       "    'citric_acid' (column 3)\n",
       "    'fixed_acidity' (column 4)\n",
       "    'residual_sugar' (column 5)\n",
       "    'chlorides' (column 6)\n",
       "    'density' (column 7)\n",
       "    'total_sulfur_dioxide' (column 8)\n",
       "    'volatile_acidity' (column 9)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec221aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add6adf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., ..., 6., 5., 6.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_new.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7c14978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DesignMatrix with shape (1599, 5)\n",
       "  Intercept  alcohol  sulphates  citric_acid  fixed_acidity\n",
       "          1      9.4       0.56         0.00            7.4\n",
       "          1      9.8       0.68         0.00            7.8\n",
       "          1      9.8       0.65         0.04            7.8\n",
       "          1      9.8       0.58         0.56           11.2\n",
       "          1      9.4       0.56         0.00            7.4\n",
       "          1      9.4       0.56         0.00            7.4\n",
       "          1      9.4       0.46         0.06            7.9\n",
       "          1     10.0       0.47         0.00            7.3\n",
       "          1      9.5       0.57         0.02            7.8\n",
       "          1     10.5       0.80         0.36            7.5\n",
       "          1      9.2       0.54         0.08            6.7\n",
       "          1     10.5       0.80         0.36            7.5\n",
       "          1      9.9       0.52         0.00            5.6\n",
       "          1      9.1       1.56         0.29            7.8\n",
       "          1      9.2       0.88         0.18            8.9\n",
       "          1      9.2       0.93         0.19            8.9\n",
       "          1     10.5       0.75         0.56            8.5\n",
       "          1      9.3       1.28         0.28            8.1\n",
       "          1      9.0       0.50         0.08            7.4\n",
       "          1      9.2       1.08         0.51            7.9\n",
       "          1      9.4       0.53         0.48            8.9\n",
       "          1      9.7       0.65         0.31            7.6\n",
       "          1      9.5       0.91         0.21            7.9\n",
       "          1      9.4       0.53         0.11            8.5\n",
       "          1      9.7       0.63         0.14            6.9\n",
       "          1      9.3       0.56         0.16            6.3\n",
       "          1      9.5       0.59         0.24            7.6\n",
       "          1      9.5       0.91         0.21            7.9\n",
       "          1      9.4       0.55         0.00            7.1\n",
       "          1      9.8       0.59         0.00            7.8\n",
       "  [1569 rows omitted]\n",
       "  Terms:\n",
       "    'Intercept' (column 0)\n",
       "    'alcohol' (column 1)\n",
       "    'sulphates' (column 2)\n",
       "    'citric_acid' (column 3)\n",
       "    'fixed_acidity' (column 4)\n",
       "  (to view full data, use np.asarray(this_obj))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3eb816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.4 ,  0.56,  0.  ,  7.4 ],\n",
       "       [ 9.8 ,  0.68,  0.  ,  7.8 ],\n",
       "       [ 9.8 ,  0.65,  0.04,  7.8 ],\n",
       "       ...,\n",
       "       [11.  ,  0.75,  0.13,  6.3 ],\n",
       "       [10.2 ,  0.71,  0.12,  5.9 ],\n",
       "       [11.  ,  0.66,  0.47,  6.  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = X[:, 1:]\n",
    "X_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7818997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c41046",
   "metadata": {},
   "source": [
    "# 참조\n",
    "## https://github.com/vikrantkakad/Red-Wine-Quality-Analysis/blob/master/Capstone_Vikrant.Kakad.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d8b18",
   "metadata": {},
   "source": [
    "# Self analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbaa4eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed_acidity         1599 non-null   float64\n",
      " 1   volatile_acidity      1599 non-null   float64\n",
      " 2   citric_acid           1599 non-null   float64\n",
      " 3   residual_sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free_sulfur_dioxide   1599 non-null   float64\n",
      " 6   total_sulfur_dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c794fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = wine.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9314cba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.4  ,  0.7  ,  0.   , ...,  3.51 ,  0.56 ,  9.4  ],\n",
       "       [ 7.8  ,  0.88 ,  0.   , ...,  3.2  ,  0.68 ,  9.8  ],\n",
       "       [ 7.8  ,  0.76 ,  0.04 , ...,  3.26 ,  0.65 ,  9.8  ],\n",
       "       ...,\n",
       "       [ 6.3  ,  0.51 ,  0.13 , ...,  3.42 ,  0.75 , 11.   ],\n",
       "       [ 5.9  ,  0.645,  0.12 , ...,  3.57 ,  0.71 , 10.2  ],\n",
       "       [ 6.   ,  0.31 ,  0.47 , ...,  3.39 ,  0.66 , 11.   ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = data_train[:, :-1]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3218f78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., ..., 6., 5., 6.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = data_train[:, -1]\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475225b",
   "metadata": {},
   "source": [
    "# 암것도 안하고 회귀 분석. 이후에 양의 상관계수를 가진 특성들만 가지고 훈련해볼 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcde29e",
   "metadata": {},
   "source": [
    "## 1-1. LogisticRegression [solver=\"liblinear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69475f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.81009259 0.71589105 0.70267347 0.79843597 0.76239753 0.6846532\n",
      " 0.66143783 0.75415516 0.72886899 0.72684378]\n",
      "Mean: 0.7345449564089788\n",
      "Standard deviation: 0.04498772825312228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10, random_state=42)\n",
    "log_reg_score = cross_val_score(log_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "log_rmse_score = np.sqrt(-log_reg_score)\n",
    "display_score(log_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd21b2",
   "metadata": {},
   "source": [
    "## 1-2. LogisticRegression [C=5 (decrease)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1150dcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.79056942 0.73314391 0.71589105 0.78262379 0.76239753 0.70710678\n",
      " 0.66614563 0.74582169 0.70710678 0.7004042 ]\n",
      "Mean: 0.731121079193854\n",
      "Standard deviation: 0.03733056833212232\n"
     ]
    }
   ],
   "source": [
    "log_reg2 = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_reg_score2 = cross_val_score(log_reg2, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "log_rmse_score2 = np.sqrt(-log_reg_score2)\n",
    "display_score(log_rmse_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c175b",
   "metadata": {},
   "source": [
    "## display_score 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed606cf1",
   "metadata": {},
   "source": [
    "## 2. SVM Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde9d024",
   "metadata": {},
   "source": [
    "# 2-1. Using kernel SVM Model 이건 오래걸리므로 나중에 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import LinearSVR\n",
    "# from sklearn.svm import SVR\n",
    "# svm_poly = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"auto\")\n",
    "\n",
    "# svm_poly_score = cross_val_score(svm_poly, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "# svm_poly_rmse = np.sqrt(-svm_poly_score)\n",
    "# display_score(svm_poly_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score(svm_poly_reg, X_train, y_train, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b60ffe",
   "metadata": {},
   "source": [
    "### 2-2. Using Linear SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42527c76",
   "metadata": {},
   "source": [
    "#### svm_reg.fit(X, y) 빼고 바로 cross_val_score 돌리면 안대냐? ;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe63f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.85553596 0.64885695 0.78267117 0.71928669 0.61822008 0.7446786\n",
      " 0.63665214 0.65463868 0.61398251 0.72416424]\n",
      "Mean: 0.6998687034651347\n",
      "Standard deviation: 0.07533327592039181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5, random_state=42, max_iter = 10000)\n",
    "svm_reg_score = cross_val_score(svm_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "svm_rmse_score = np.sqrt(-svm_reg_score)\n",
    "display_score(svm_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73849f31",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43f09a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.62545082 0.61762548 0.67620986 0.67963328 0.57786445 0.71376148\n",
      " 0.67820201 0.64711995 0.6045297  0.7012463 ]\n",
      "Mean: 0.6521643328204947\n",
      "Standard deviation: 0.04235027764402308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(random_state=42)\n",
    "gbrt_score = cross_val_score(gbrt, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "gbrt_rmse_score = np.sqrt(-gbrt_score)\n",
    "display_score(gbrt_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9286f",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09386064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.61371614 0.58156631 0.68472075 0.66384722 0.5649209  0.72317443\n",
      " 0.66583876 0.65855239 0.61977667 0.70264281]\n",
      "Mean: 0.6478756369545334\n",
      "Standard deviation: 0.04894232181514159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rnd_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_reg_score = cross_val_score(rnd_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "rnd_rmse_score = np.sqrt(-rnd_reg_score)\n",
    "display_score(rnd_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae4259",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de367490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.73529954 0.71168031 0.79269341 0.76499069 0.63282562 0.79991351\n",
      " 0.74242189 0.74317426 0.66748292 0.76945147]\n",
      "Mean: 0.7359933619216152\n",
      "Standard deviation: 0.05028193557207711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2, random_state = 42)\n",
    "tree_reg_score = cross_val_score(tree_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_score = np.sqrt(-tree_reg_score)\n",
    "display_score(tree_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1178be",
   "metadata": {},
   "source": [
    "# Bagging Regressor (using R-F Regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 244, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\", line 370, in _fit\n",
      "    all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1054, in __call__\n",
      "    self.retrieve()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 933, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\", line 439, in result\n",
      "    return self.__get_result()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\", line 388, in __get_result\n",
      "    raise self._exception\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "bag_reg = BaggingRegressor(\n",
    "    RandomForestRegressor(random_state=42), n_estimators = 500,\n",
    "    max_samples = 100, bootstrap = True, n_jobs = -1, random_state = 42)\n",
    "bag_score = cross_val_score(bag_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "bag_rmse_score = np.sqrt(-bag_score)\n",
    "display_score(bag_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b3be1",
   "metadata": {},
   "source": [
    "# Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca8c2c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [nan nan nan nan nan nan nan nan nan nan]\n",
      "Mean: nan\n",
      "Standard deviation: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 485, in fit\n",
      "    return super().fit(X, y, sample_weight)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\", line 66, in fit\n",
      "    names, clfs = self._validate_estimators()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 241, in _validate_estimators\n",
      "    raise ValueError(\n",
      "ValueError: The estimator LogisticRegression should be a regressor.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import VotingRegressor\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "# rnd_reg = RandomForestRegressor(random_state=42)\n",
    "# svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
    "\n",
    "# voting_reg = VotingRegressor(\n",
    "#     estimators=[('lr', log_reg), ('rf', rnd_reg), ('svc', svm_reg)])\n",
    "\n",
    "# voting_score = cross_val_score(voting_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "# voting_rmse_score = np.sqrt(-voting_score)\n",
    "# display_score(voting_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4377d4",
   "metadata": {},
   "source": [
    "## 3. AdaBoost는 개별 분류기 다 돌려보고 성능 좋은걸로 돌립시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d90aa5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.64066127 0.6053048  0.67202026 0.64597068 0.53605835 0.71734926\n",
      " 0.68753033 0.66042165 0.59559969 0.69106477]\n",
      "Mean: 0.64519810637125\n",
      "Standard deviation: 0.05102635304230313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_reg = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(max_depth=5), n_estimators=1000,\n",
    "    learning_rate = 0.01, random_state=42)\n",
    "ada_reg_score = cross_val_score(ada_reg, X_train, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "ada_rmse_score = np.sqrt(-ada_reg_score)\n",
    "display_score(ada_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f664c",
   "metadata": {},
   "source": [
    "# 이제부터 음의 상관계수를 띄는 특성들을 제거하고 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2d722e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1baff6b9f438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlog_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"liblinear\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlog_reg_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"neg_mean_squared_error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlog_rmse_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlog_reg_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdisplay_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_rmse_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10, random_state=42)\n",
    "log_reg_score = cross_val_score(log_reg, X_train_new, y_train, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "log_rmse_score = np.sqrt(-log_reg_score)\n",
    "display_score(log_rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6b4a3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.68709093 0.63585793 0.67414365 0.68678506 0.57852584 0.75033954\n",
      " 0.65457202 0.70013245 0.62911238 0.73891868]\n",
      "Mean: 0.6735478480508899\n",
      "Standard deviation: 0.04901056106841354\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(random_state=42)\n",
    "gbrt_score = cross_val_score(gbrt, X_train_new, y_train, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "gbrt_rmse_score = np.sqrt(-gbrt_score)\n",
    "display_score(gbrt_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e28a3",
   "metadata": {},
   "source": [
    "## GitHub 참조 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f84c9dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.8253787  0.77862058 0.72024301 0.85146932 0.73739406 0.6846532\n",
      " 0.68920244 0.77459667 0.698212   0.76889204]\n",
      "Mean: 0.7528662014620664\n",
      "Standard deviation: 0.0541939069584882\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = sklearn.linear_model.LogisticRegression(max_iter = 5000, random_state=42)\n",
    "y, X = dmatrices('quality ~ alcohol + sulphates + citric_acid + fixed_acidity', data = wine)\n",
    "# model.fit(X, y.ravel())\n",
    "# cross_val_score(model, X, y.ravel(), cv=10, scoring=\"accuracy\")\n",
    "log_score = cross_val_score(log_reg, X, y.ravel(), scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "log_rmse_score = np.sqrt(-log_score)\n",
    "display_score(log_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5dd674",
   "metadata": {},
   "source": [
    "# Shuffle 이후 다시 해봅시다\n",
    "### X_train => X_shuffle // y_train -> y_shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49ff6a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.63245553 0.75       0.74161985 0.77862058 0.75828754 0.6846532\n",
      " 0.75       0.70267347 0.72456884 0.72250438]\n",
      "Mean: 0.724538338023898\n",
      "Standard deviation: 0.04025487045318829\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver=\"liblinear\", C=10, random_state=42)\n",
    "log_reg_score = cross_val_score(log_reg, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "log_rmse_score = np.sqrt(-log_reg_score)\n",
    "display_score(log_rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6a5514bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.63917241 0.60097211 0.622932   0.56342159 0.60216987 0.61328411\n",
      " 0.62342671 0.68386438 0.61197319 0.60022336]\n",
      "Mean: 0.616143972670955\n",
      "Standard deviation: 0.029495132856633105\n"
     ]
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(random_state=42)\n",
    "gbrt_score = cross_val_score(gbrt, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "gbrt_rmse_score = np.sqrt(-gbrt_score)\n",
    "display_score(gbrt_rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c67909f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.48413841 0.55282965 0.51164868 0.52362439 0.56412709 0.62620683\n",
      " 0.60666455 0.56633747 0.61227343 0.57928714]\n",
      "Mean: 0.5627137637456441\n",
      "Standard deviation: 0.04372231789118146\n"
     ]
    }
   ],
   "source": [
    "rnd_reg = RandomForestRegressor(n_estimators = 100, random_state=42)\n",
    "rnd_reg_score = cross_val_score(rnd_reg, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "rnd_rmse_score = np.sqrt(-rnd_reg_score)\n",
    "display_score(rnd_rmse_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de43662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.62271523 0.52321497 0.55727388 0.51158306 0.55095288 0.56611497\n",
      " 0.58607875 0.68311766 0.53490312 0.56825423]\n",
      "Mean: 0.5704208752831044\n",
      "Standard deviation: 0.048173968164455176\n"
     ]
    }
   ],
   "source": [
    "rnd_reg = RandomForestRegressor(n_estimators = 1000, oob_score=True, random_state=42)\n",
    "rnd_reg_score = cross_val_score(rnd_reg, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "rnd_rmse_score = np.sqrt(-rnd_reg_score)\n",
    "display_score(rnd_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afc44c1",
   "metadata": {},
   "source": [
    "## Using GradientBoosting. less than R-F Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "924604f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.66190794 0.58670547 0.59029032 0.5577623  0.62219569 0.61504266\n",
      " 0.64881921 0.71083583 0.57486387 0.58347325]\n",
      "Mean: 0.6151896533754406\n",
      "Standard deviation: 0.044578211487857386\n"
     ]
    }
   ],
   "source": [
    "gdb_reg = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt',\n",
    "                                    min_samples_leaf=15, min_samples_split=10, loss='huber')\n",
    "gdb_score = cross_val_score(gdb_reg, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "gdb_rmse_score = np.sqrt(-gdb_score)\n",
    "display_score(gdb_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd7776",
   "metadata": {},
   "source": [
    "## Using AdaBoost. 최저값!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1425881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.61119811 0.5157646  0.5327341  0.50223376 0.54339327 0.55719891\n",
      " 0.57113921 0.67735192 0.5249994  0.56001628]\n",
      "Mean: 0.5596029578675488\n",
      "Standard deviation: 0.04905204832890824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "ada_reg = AdaBoostRegressor(\n",
    "    RandomForestRegressor(random_state=42), n_estimators=1000,\n",
    "    learning_rate = 0.01, random_state=42)\n",
    "ada_reg_score = cross_val_score(ada_reg, X_shuffle, y_shuffle, scoring = \"neg_mean_squared_error\", cv=10)\n",
    "ada_rmse_score = np.sqrt(-ada_reg_score)\n",
    "display_score(ada_rmse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d3a0a",
   "metadata": {},
   "source": [
    "## AdaBosst가 0.57 이상 나온다면...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ea2ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    train_errors, val_errors =[], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "        \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee2aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
